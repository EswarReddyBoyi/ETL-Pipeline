ETL Pipeline using Pandas & Scikit-Learn

This project implements an ETL (Extract â†’ Transform â†’ Load) pipeline using Pandas and Scikit-Learn.
The pipeline automates dataset preprocessing including missing value handling, encoding categorical features, scaling numerical features, splitting into train/test sets, and saving processed data.

Features

Extract â†’ Reads raw dataset (CSV).

Transform â†’

Handles missing values (SimpleImputer).

Scales numerical columns (StandardScaler).

Encodes categorical columns (OneHotEncoder).

Splits data into training/testing sets.

Load â†’ Saves processed data as CSV files in processed_data/.

Project Structure
PROJECT/
â”‚
â”œâ”€â”€ etl_pipeline.py       # Main ETL script
â”œâ”€â”€ README.md             # Documentation
â”œâ”€â”€ titanic.csv           # Example dataset (raw data)
â””â”€â”€ processed_data/       # Output folder with transformed datasets
    â”œâ”€â”€ X_train.csv
    â”œâ”€â”€ X_test.csv
    â”œâ”€â”€ y_train.csv
    â””â”€â”€ y_test.csv

How to Run
1. Clone or download the project
git clone <your-repo-link>
cd PROJECT

2. Install dependencies
pip install pandas scikit-learn

3. Download example dataset

Titanic dataset (CSV):
Download Link

Save it as titanic.csv inside the project folder.

4. Run the pipeline
python etl_pipeline.py

âœ… Output

After running, processed files will be saved in processed_data/:

X_train.csv â†’ Training features

X_test.csv â†’ Testing features

y_train.csv â†’ Training labels

y_test.csv â†’ Testing labels

ğŸ› ï¸ Customization

Change dataset file path inside etl_pipeline.py:

raw_file = "titanic.csv"
target_col = "Survived"


Replace with your dataset and target column.

Example

Using Titanic dataset (titanic.csv, target = Survived):

Raw shape: 891 rows Ã— 12 columns

Transformed â†’ Train/Test split (80/20).

Saved as new CSVs in processed_data/.